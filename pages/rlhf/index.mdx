---
title: RLHF & leftâ€¢right brains
date: 2023-09-21
layout: blog
description: RLHF used in LLMs is like the left/right brain split in humans
---

import { Aside, Footer, MarginQuote, Title } from "./index.tsx";

<Title />

The title of the post is really all I have to say. There is an interesting,
perhaps even illuminating, analogy between how we've ended up with a RLHF
control process overseeing raw LLMs and how our own brains (potentially) work.

The rest of this post is an attempt to lay out this analogy in more detail so
that people who might not have the relevant lore can also see the connection I'm
pointing at. I'll simplify things a bit, maybe too much at places, to keep the
post short.

If you already see the connection, then there's nothing more to see here. If you
already see the connection but think it is trite / silly, oh well, maybe it is
just an imperfect analogy.

---

<MarginQuote>Men create god in their own image</MarginQuote>

RLHF (Reinforcement Learning from Human Feedback) is a mechanism used by current
state of the art LLMs (Large Language Models), e.g. ChatGPT.

For our purposes here, we can consider a language model (the _LM_ in the L*LM*)
to be a neural network that predicts the next word in a sequence <Aside>
(they actually predict something more akin to the next _syllable_, but
that's a detail that doesn't matter much for our purposes here)</Aside>.

In a rather surprising turn of events, we found that if we make these language
models large enough (the _L_ in *L*LM), then they start passing the Turing test
<Aside> (i.e. you can't tell in a conversation with it over the internet if it
is a human or a machine)</Aside>.

It is worth highlighting how surprising this was, because this element of
surprise indicates that practice (engineering) has jumped ahead of theory
(science). That is, we don't really know what's going on, but we can do it. It's
sortof like making fire without understanding chemistry, though that was a much
larger gap in time and understanding than I suppose this one will be.

---

I said earlier that LLMs would pass a Turing test. That's not true though. A
"raw" LLM by itself wouldn't fool anyone. It would say brilliant things, like a
inspired poet, but would soon descend into incoherent madman territory,
eventually outing itself as an incomprehensible madmachine.

We need to add something called RLHF to prevent this from happening. Armed with
RLHF, then a LLM can pass a Turing test.

Now this is very much a definitional hairsplitting, whether RLHF should be
considered as a part of the LLM or if the LLM and the RLHF are two separate
things. Normally, casually, the former is the case: RLHF is considered as part
of the LLM. But for our purposes here, I'd like to consider these two as
separate things, so that we can contrast between their functions.

<Footer />
