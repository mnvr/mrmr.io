---
title: RLHF & leftâ€¢right brains
date: 2023-09-21
layout: blog
description: RLHF used in LLMs is like the left/right brain split in humans
---

import * as I from "./index.tsx";

<I.Title />

The title of the post is really all I have to say. There is an interesting,
perhaps even illuminating, analogy between how we've ended up with a RLHF
control process overseeing raw LLMs and how our own brains (potentially) work.

The rest of this post is an attempt to lay out this analogy in more detail so
that people who might not have the relevant lore can also see the connection I'm
pointing at. I'll simplify things a bit, maybe too much at places, to keep the
post short.

If you already see the connection, then there's nothing more to see here. If you
already see the connection but think it is trite / silly, oh well, maybe it is
just an imperfect analogy.

<I.HR2d />

<I.MarginQuote>Men create god in their own image</I.MarginQuote>

RLHF (Reinforcement Learning from Human Feedback) is a mechanism used by current
state of the art LLMs (Large Language Models), e.g. ChatGPT.

For our purposes here, we can consider a language model (the _LM_ in the L*LM*)
to be a neural network that predicts the next word in a sequence <I.Aside>
(they actually predict something more akin to the next _syllable_, but
that's a detail that doesn't matter much for our purposes here)</I.Aside>.

In a rather surprising turn of events, we found that if we make these language
models large enough (the _L_ in *L*LM), then they start passing the Turing
test<I.Aside>(i.e. you can't tell in a conversation with it over the internet if
it is a human or a machine)</I.Aside>.

It is worth highlighting how surprising this was, because this element of
surprise indicates that practice (engineering) has jumped ahead of theory
(science). That is, we don't really know what's going on, but we can do it. It's
sortof like making fire without understanding chemistry, though that was a much
larger gap in time and understanding than I suppose this one will be.

<I.HRT />

I said earlier that LLMs would pass a Turing test. That's not true though. A
"raw" LLM by itself wouldn't fool anyone. It would say brilliant things, like a
inspired poet, but would soon descend into incoherent madman territory,
eventually outing itself as an incomprehensible madmachine.

We need to add something called RLHF to prevent this from happening. Armed with
RLHF, then a LLM can pass a Turing test.

Now this is very much a definitional hairsplitting, whether RLHF should be
considered as a part of the LLM or if the LLM and the RLHF are two separate
things. Normally, casually, the former is the case: RLHF is considered as part
of the LLM. But for our purposes here, I'd like to consider these two as
separate things, so that we can contrast between their functions.

<I.HRT />

So we have a LLM that given a sequence of text, predicts the next word that is
the most likely. Most likely in which sense? Suppose that we get 10 people in a
room, give them a sequence of text, let us say

<I.Example>I like to ___</I.Example>

and ask them to fill in the blank - predict the next word in the sequence.
What'll they fill in, I ask you?

Well, yes, the answer is not possible to guess since this is a very subjective
ask. The word here will depend a lot on the idiosyncracies of these 10
individuals, on their entire life history. For many of them, I'll wager that
word they fill in ("predict") will change over time, if we were to ask them
again later.

But what if we get 10 thousand people in a room? Or 10 million of them?

Firstly we'll need bigger rooms. Secondly, if there is some homogenity in the
way these 10 million people are selected from amongst the (almost) 10 billion
people on our planet, we will start to see some statistical regularity in the
answers. In some cases where there is extreme homogenity, there might even be a
clear cut majority answer.

LLMs are like this. They're as if we put 10 million people in a room, and ask
them to fill in the next word of a sequence. Just how the prediction of the 10
million people depends on how those people were picked, similarily the
prediction of the LLM depends on the data it was trained on.

Our example sentence thus highlights two aspects of LLMs

-   There is no one right answer (i.e. "best prediction" / next word in the
    sequence).

-   The LLMs response ("personality" so as to say) depends on the data it was
    trained on.

But I dislike my example sentence because to people that might not be aware of
the underlying mechanics, it might give off the wrong impression that LLMs
"just" memorize the data set they're trained on and parrot off the most common
next word. That's very much not the case.

**LLMs can understand our language.** We know this because we can ask them to
predict the next words for text sequences that are not even remotely in the data
set, and as far as we can tell, they can understand our _intent_.

They might not
be able to predict the correct word (which would imply that
that particular LLM is maybe not too intelligent)

consciousness, the thing we can't seem to be able to define but that we are
absolutely certain that exists, because it is indeed the only thing we be
certain that exists.

<I.Footer />
