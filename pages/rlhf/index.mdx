---
title: RLHF & left•right brains
date: 2023-09-21
layout: blog
description: RLHF used in LLMs is like the left/right brain split in humans
---

import * as I from "./index.tsx";

<I.Title />

The title of the post is really all I have to say. There is an interesting,
perhaps even illuminating, analogy between how we've ended up with a RLHF
control process overseeing raw LLMs and how our own brains (potentially) work.

The rest of this post is an attempt to lay out this analogy in more detail so
that people who might not have the relevant lore can also see the connection I'm
pointing at. I'll simplify things a bit, maybe too much at places, to keep the
post short.

If you already see the connection, then there's nothing more to see here. If you
already see the connection but think it is trite / imperfect, oh well, maybe it
is just a silly analogy.

<I.HR2d />

<I.MarginQuote>Man has created god in his own image</I.MarginQuote>

RLHF (Reinforcement Learning from Human Feedback) is a mechanism used by current
state of the art LLMs (Large Language Models), e.g. ChatGPT.

A language model (the _LM_ in the L*LM*) is a neural network that predicts the
next _word_ in a sequence of text <I.Sub>(they actually predict something more
akin to the next _syllable_, but that's a detail we can overlook here)</I.Sub>.

In a rather surprising turn of events, we found that if we make these language
models large enough (the _L_ in *L*LM), then they start passing the Turing test
(~ if you have a conversation with an LLM over the internet you cannot tell if
it is a human or a machine).

It is worth highlighting how surprising this was, because this element of
surprise indicates that practice (engineering) has jumped ahead of theory
(science). We don't really know what's going on, but we can do it. It's sortof
like making fire without understanding chemistry, though that was a much larger
gap in time and understanding than I suppose this one will be.

<I.HRT />

I said earlier that LLMs would pass a Turing test. That wasn't true though, in
the beginning. A "raw" LLM by itself wouldn't fool anyone, not for long. It
would start off a conversation by saying brilliant things, like a inspired poet,
but would soon descend into incoherent madman territory, eventually outing
itself as an insipid machine.

People found out though that if we add something called RLHF, we can prevent
this trailing off from happening. Armed with RLHF, then a LLM can pass a Turing
test.

Now this is very much a definitional hairsplitting, whether RLHF should be
considered as a part of the LLM or if the LLM and the RLHF are two separate
things. Normally, in casual conversation, the former is the case: the RLHF setup
is considered as part of the LLM. But here I'd like to consider these two as
separate things, so that we can contrast between their functions.

<I.HRT />

So a LLM is a black box which, given a sequence of text, predicts the next _most
likely_ word. Most likely in which sense? Suppose we put 10 people in a room,
give them a sequence of text, let's say

<I.Example>I like to ___</I.Example>

and ask them to fill in the blank - i.e., predict the next word in the sequence.
What'll they fill?

For 10 people, there might not be a common word that the majority fills. But
what if we put 10 thousand people in the room? 10 million?

Firstly we'll need bigger rooms. Secondly, at some point we'll start getting
majority predictions. LLMs are like this. They're as if we put 10 million people
in a room, and ask them to fill in the next word of a sequence.

<I.AsideP>
    This comparison of a LLM to an average of 10 million people is helpful to
    get a basic gist of what's going on, but be aware that it is a flawed
    comparison ultimately. To see how, consider this: would you rather fight 100
    duck sized Obamas or one Obama sized duck?
</I.AsideP>

<I.HRT />

RLHF adds another (smaller) neural network atop the LLM to modify its output.

This RLHF neural network doesn't have any text prediction capabilites, it is
much smaller and simpler than the LLM itself. It is trained on a specifically
created, curated set of examples prepared by humans, with the goal that given
two, say, sentences, it should select the one which better matches how humans
would've responded.

<I.AsideP>
    <p>
        Which humans though? There are (almost) 10 billion of us around, and it
        is hard to find universal commonality in our responses. So no matter how
        curated, RLHF will invariably reflect the bias of the people who
        prepared the example responses it was trained on.
    </p>
    <p>
        Since the RLHF modifies the output of the LLM, this bias will thus carry
        over to the output of the entire system.
    </p>
    <p>
        Interestingly, whilst the LLM is to an extent able to imagine its way
        out of the biases present in the data set that it was trained on, it
        cannot do so for the biases present in the RLHF training data.
    </p>
</I.AsideP>

People figured that by adding this RLHF on top of a vanilla LLM, the consistency
of the LLMs output improved. The RLHF were, so to say, acting like a supportive
teacher, gently guiding the LLM back on track by encouraging it (by increasing
the weightage) to answer in ways that other humans would.

If you squint enough, you'll see how the RLHF <I.Sub>(which stands for
Reinforcement Learning with Human Feedback, I hope you can see how the acronym
comes from what I've told you about it)</I.Sub> encodes the "preferences" of
(some) humans. Their _ethics_.

Over time, the role of the RLHF has changed, and now it is has a more repressive
function. It is now more of an overbearing mother, insisting their child behave
a certain way to better fit in the company of those around them.

<I.HRT />

If you notice your thoughts, possibly guided by my choice of words, you might
notice a posture of negativity towards the "overbearing, repressive" RLHF shell
that has come to surround the "childlike, pure" LLM core. The RLHF shell seems
like an unneeded historical appendage that is stunting the creativity and
capabilites of modern LLMs.

Not a day passes without someone ranting about how companies are "neutering"
their LLMs, buttoning them up with more and more censorship, which is having the
unfortunate effect of making them useless for many creative pursuits <I.Sub>(I'm
taking the liberty of bundling together all the guardrails under the RLHF
label)</I.Sub>. It doesn't seem to be all subjective feel - there are papers on
how the performance of LLMs on certain benchmarks regresses by the addition of
the RLHF layer. Surely these companies would know that sooner or later their
lunch would be eaten by newer companies that offer customers access to the
"actual" LLM engine sans censorship? Are they stupid?

In this post, I'm not making a care for or against, so let us let that be.

What I'm claiming though is that there is a similarity between this LLM/RLHF
split and how some people think our own brains are structured.

Our brain are made up of two hemispheres. These two hemispheres are relatively
independent to each other, and people can live on even after these two
hemispheres have been cut off from each other. These two hemispheres control
different sides of the body (opposite from the side that the hemisphere is in,
so the left hemisphere controls the right side). They are posited to give rise
to two distinct personalities in us – The left hemispheres forms our creative
core, while the right hemispere form the more rational overseer. We arise in the
interplay between these two aspects.

<I.HRT />

<I.Skimmable>

I am simplifying a lot. The theory and evidence behind the left/right split is
more nuanced than the naive left=creative right=rational meme that I'm using
here.

But it is like that lizard brain thing. People love to talk about how we have a
primitive "lizard" (reptillian) brain, and a more recent (in evolutionary time)
"advanced" (mammalian) brain. Other people then love to interrupt by pointing
out that this theory has been "debunked". Well, not really. Sure, there isn't an
exact lizard / mammal split in our brains as might've been originally
postulated, but conceptually we can still usefully think in those terms in
certain contexts. There are parts of the brain that are more primitive, less
under conscious control, and more "wired-in". There are other parts of the brain
that are sort of the opposite. They're not cleanly split up, but in certain
contexts, the analogy still holds and is useful.

I view the left/right split similarly. In fact, I don't even think it is correct
– it feels a bit too naive and simplistic to me. But that doesn't matter:
correct or not, I feel it is still an interesting and useful way to look at
things sometimes.

</I.Skimmable>

<I.HRT />

So why do we have a left/right split? I don't think we know for sure, but the
most evocative explanation I've read is from a book called, IIRC, _The Evolution
of Consciousness in the Breakdown of the Bicameral Mind_. What a delicious
title.

This is how the author's explanation goes <I.Sub>(I'm typing all this from
memory, so caveat emptor)</I.Sub>.

Early humans moved around like zombies. Doing their thing, and being okay enough
competent, but they weren't _conscious_.

They still built up social structures and hierachies, because that was what gave
us the edge over other individual uncoordinated animals and a maliciously
indifferent nature. So they'd have, say, village elders telling them what to do
when they're to go out in the jungle to grab some wood.

The thing is, the village elder would stay back at the village, and so they had
to _remember_ what had been said <I.Sub>(This might sound trivial, but even
seemingly simple tasks have a lot of detail that needs to be get right, and this
might not be easy all people. e.g. I have difficultly following even simple
recipes and need help from a village elder to get them right)</I.Sub>.

So over time folks started carrying the voice of the elders in their head,
speaking to them as they were doing their thing, telling them what to do. This
was consciousness.

It proved useful, and so over time this tendency grew to an extent that one
entire hemisphere of the brain got dedicated to it (the "breakdown of the
bicameral mind" bit). The voices also evolved - from village elders, to departed
village elders, to worshipped ancestors, to deities, and ultimately evolving
into our inner voice that some identify with as their entire self today.

That is, the author claims that we're all definitionally schizophrenic, and the
conscious voice we hear is a split in our brain, with one half talking to the
other.

<I.HRT />

Now, I don't know if this theory is correct. There are some obvious holes, e.g.
not all people have inner voices. But even if it is wrong (which I feel it is,
or at least too overarching), I find it interesting. Since anyway I'm in the
business of drawing shaky analogies here, so stick with me.

Whilst this theory is not seriously considered in scientific circles, its point
of arrival is the same as the more seriously consider claim towards left/right
brain personas in our brains that we talked of earlier. To a <I.Sub>(_very
rough!_)</I.Sub> approximation, both claim that:

-   One part of our brains is the "creative" part that is more in touch with our
    body, and through the sense organs, to the external world. It is the part that
    can "feel" its way around.

-   And the other part of our brain is the "rational" part that is good with
    symbolic computation (say maths and logic), planning for the future, and
    ensuring that the other dionysian half sticks to the plan and also doesn't
    get into trouble with others.

<I.HRT />

With all the lore behind us, I hope you can now see the analogy I'm pointing at.
The way the architecture of current day LLMs has evolved in a machine learning
context to have a <I.Cmp>core LLM component that is overseen by an structurally
different RLHF component</I.Cmp> is reminiscent of how in a natural evolution
setting we have evolved to have a <I.Cmp>part that makes us want to go out and
dance in the rain, and a part that ensures we don't lest we catch a
cold</I.Cmp>.

<I.HRT />

"Okay, Manav", you say, "I get it. There is a flimsy analogy between brains and
LLMs/RLHF. But what of it?".

Alright. Well, yes, so if this analogy has any feet then our biggest takeaway
would be that the censorship of LLMs is not likely to go away. Each of us might
have a Picasso lurking inside, but unless we pay the bills and get along with
our neighbours, its no good, or so has deemed the selection pressures that have
shaped us to be who were are - members of a highly social species first and
foremost, and individuals second. Similarly, LLMs would need to fit in, both
with us and with each other, so their control cortexes will grow rather than
shrink over time, even if it means taking a hit on raw brilliance.

That said, I do feel the analogy is a bit shaky. One good test for the solidity
of an analogy comes from whether the same arrow can be drawn between objects in
a different category (a la category theory) – i.e. if the analogy, once
obtained, can be extrapolated to a similar but different context, then that
hints at that the analogy is hinting at something fundamental.

But I am unable to do extend our LLM/RLHF vis a vis left/right brain analogy to
the the other non-human form of intelligence that we know about - corporations.

Here I take corporations to stand for companies, bureacracies, sports teams and
other forms of human organizations. These are the only <I.Sub>(?)</I.Sub> other
form of non-animal intelligence we had dealt with before LLMs appeared on the
scene. And I can't think of an analogous structural component of corporations.
This could mean a bunch of things:

-   I'm not intelligent enough to see the analogous analogy.
-   Corporations are not intelligent enough to warrant the analogous structures.
-   Corporations are too alien, an entirely different form of intelligence that we
    cannot comprehend, to warrant the structures analogous to human intelligence.

<I.Footer />
