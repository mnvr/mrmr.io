---
title: RLHF & leftâ€¢right brains
date: 2023-09-21
layout: blog
description: RLHF used in LLMs is like the left/right brain split in humans
---

import * as I from "./index.tsx";

<I.Title />

The title of the post is really all I have to say. There is an interesting,
perhaps even illuminating, analogy between how we've ended up with a RLHF
control process overseeing raw LLMs and how our own brains (potentially) work.

The rest of this post is an attempt to lay out this analogy in more detail so
that people who might not have the relevant lore can also see the connection I'm
pointing at. I'll simplify things a bit, maybe too much at places, to keep the
post short.

If you already see the connection, then there's nothing more to see here. If you
already see the connection but think it is trite / silly, oh well, maybe it is
just an imperfect analogy.

<I.HR2d />

<I.MarginQuote>Men create god in their own image</I.MarginQuote>

RLHF (Reinforcement Learning from Human Feedback) is a mechanism used by current
state of the art LLMs (Large Language Models), e.g. ChatGPT.

For our purposes here, we can consider a language model (the _LM_ in the L*LM*)
to be a neural network that predicts the next word in a sequence <I.Aside>
(they actually predict something more akin to the next _syllable_, but
that's a detail that doesn't matter much for our purposes here)</I.Aside>.

In a rather surprising turn of events, we found that if we make these language
models large enough (the _L_ in *L*LM), then they start passing the Turing
test<I.Aside>(i.e. you can't tell in a conversation with it over the internet if
it is a human or a machine)</I.Aside>.

It is worth highlighting how surprising this was, because this element of
surprise indicates that practice (engineering) has jumped ahead of theory
(science). That is, we don't really know what's going on, but we can do it. It's
sortof like making fire without understanding chemistry, though that was a much
larger gap in time and understanding than I suppose this one will be.

<I.HRT />

I said earlier that LLMs would pass a Turing test. That's not true though. A
"raw" LLM by itself wouldn't fool anyone. It would say brilliant things, like a
inspired poet, but would soon descend into incoherent madman territory,
eventually outing itself as an meirl machine.

We need to add something called RLHF to prevent this from happening. Armed with
RLHF, then a LLM can pass a Turing test.

Now this is very much a definitional hairsplitting, whether RLHF should be
considered as a part of the LLM or if the LLM and the RLHF are two separate
things. Normally, casually, the former is the case: RLHF is considered as part
of the LLM. But for our purposes here, I'd like to consider these two as
separate things, so that we can contrast between their functions.

<I.HRT />

So a LLM is a black box which when given a sequence of text, predicts the next
word that is the most likely. Most likely in which sense? Suppose that we get 10
people in a room, give them a sequence of text, let us say

<I.Example>I like to ___</I.Example>

and ask them to fill in the blank - i.e., predict the next word in the sequence.
What'll they fill?

For 10 people, there might not be a common word that the majority fills. But
what if we put 10 thousand people in the room? 10 million?

Firstly we'll need bigger rooms. Secondly, at some point we'll start getting
majority predictions. LLMs are like this. They're as if we put 10 million people
in a room, and ask them to fill in the next word of a sequence.

<I.AsideP>
    This comparison of a LLM to an average of 10 million people is helpful to
    get a basic gist of what's going on, but be aware that it is a flawed
    comparison ultimately. To see how, consider this: would you rather fight 100
    duck sized Obamas or one Obama sized duck?
</I.AsideP>

<I.HRT />

The idea behind RLHF is to add another (smaller) neural network on top of the
LLM to modify its output.

The RLHF neural network doesn't have any text prediction capabilites, it is much
smaller and simpler than the LLM itself. It is trained on a curated set of
examples prepared by humans, with the goal that given two, say, sentences,
it should select the one which better matches how humans would've responded.

<I.AsideP>
    <p>
        Which humans though? There are (almost) 10 billion of us around, and it
        is hard to find universal commonality in our responses. So no matter how
        curated, the RLHF will invariably reflect the bias of the people who
        prepared the example responses it was trained on.
    </p>
    <p>
        Since the RLHF modifies the output of the LLM, this bias will thus carry
        over to the output of the entire system.
    </p>
</I.AsideP>

It was seen that by adding this RLHF on top of a vanilla / raw LLM, the
consistency of the LLMs output improved. The RLHF were, so to say, acting like a
supportive teacher, gently guiding the LLM back on track by encouraging it (by
increasing the weightage) to answer in ways that humans would.

If you squint enough, you'll see how the RLHF <I.Aside>(which stands for
Reinforcement Learning with Human Feedback, I hope you can see how the acronym
comes from what I've told you about it)</I.Aside> encodes the "preferences" of
(some) humans. Their "value system". Their _ethics_.

<I.Footer />
