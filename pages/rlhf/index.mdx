---
title: RLHF & left•right brains
date: 2023-09-21
layout: blog
description: RLHF used in LLMs is like the left/right brain split in humans
---

import * as I from "./index.tsx";

<I.Title />

The title of the post is really all I have to say. There is an interesting,
perhaps even illuminating, analogy between how we've ended up with a RLHF
control process overseeing raw LLMs and how our own brains (potentially) work.

The rest of this post is an attempt to lay out this analogy in more detail so
that people who might not have the relevant lore can also see the connection I'm
pointing at. I'll simplify things a bit, maybe too much at places, to keep the
post short.

If you already see the connection, then there's nothing more to see here. If you
already see the connection but think it is trite / silly, oh well, maybe it is
just an imperfect analogy.

<I.HR2d />

<I.MarginQuote>Men create god in their own image</I.MarginQuote>

RLHF (Reinforcement Learning from Human Feedback) is a mechanism used by current
state of the art LLMs (Large Language Models), e.g. ChatGPT.

For our purposes here, we can consider a language model (the _LM_ in the L*LM*)
to be a neural network that predicts the next word in a sequence <I.Sub> (they
actually predict something more akin to the next _syllable_, but that's a detail
that doesn't matter much for our purposes here)</I.Sub>.

In a rather surprising turn of events, we found that if we make these language
models large enough (the _L_ in *L*LM), then they start passing the Turing
test<I.Sub>(i.e. you can't tell in a conversation with it over the internet if
it is a human or a machine)</I.Sub>.

It is worth highlighting how surprising this was, because this element of
surprise indicates that practice (engineering) has jumped ahead of theory
(science). That is, we don't really know what's going on, but we can do it. It's
sortof like making fire without understanding chemistry, though that was a much
larger gap in time and understanding than I suppose this one will be.

<I.HRT />

I said earlier that LLMs would pass a Turing test. That's not true though. A
"raw" LLM by itself wouldn't fool anyone. It would say brilliant things, like a
inspired poet, but would soon descend into incoherent madman territory,
eventually outing itself as an meirl machine.

We need to add something called RLHF to prevent this from happening. Armed with
RLHF, then a LLM can pass a Turing test.

Now this is very much a definitional hairsplitting, whether RLHF should be
considered as a part of the LLM or if the LLM and the RLHF are two separate
things. Normally, casually, the former is the case: RLHF is considered as part
of the LLM. But for our purposes here, I'd like to consider these two as
separate things, so that we can contrast between their functions.

<I.HRT />

So a LLM is a black box which when given a sequence of text, predicts the next
word that is the most likely. Most likely in which sense? Suppose that we get 10
people in a room, give them a sequence of text, let us say

<I.Example>I like to ___</I.Example>

and ask them to fill in the blank - i.e., predict the next word in the sequence.
What'll they fill?

For 10 people, there might not be a common word that the majority fills. But
what if we put 10 thousand people in the room? 10 million?

Firstly we'll need bigger rooms. Secondly, at some point we'll start getting
majority predictions. LLMs are like this. They're as if we put 10 million people
in a room, and ask them to fill in the next word of a sequence.

<I.AsideP>
    This comparison of a LLM to an average of 10 million people is helpful to
    get a basic gist of what's going on, but be aware that it is a flawed
    comparison ultimately. To see how, consider this: would you rather fight 100
    duck sized Obamas or one Obama sized duck?
</I.AsideP>

<I.HRT />

The idea behind RLHF is to add another (smaller) neural network on top of the
LLM to modify its output.

This RLHF neural network doesn't have any text prediction capabilites, it is
much smaller and simpler than the LLM itself. It is trained on a curated set of
examples prepared by humans, with the goal that given two, say, sentences, it
should select the one which better matches how humans would've responded.

<I.AsideP>
    <p>
        Which humans though? There are (almost) 10 billion of us around, and it
        is hard to find universal commonality in our responses. So no matter how
        curated, the RLHF will invariably reflect the bias of the people who
        prepared the example responses it was trained on.
    </p>
    <p>
        Since the RLHF modifies the output of the LLM, this bias will thus carry
        over to the output of the entire system.
    </p>
</I.AsideP>

It was seen that by adding this RLHF on top of a vanilla / raw LLM, the
consistency of the LLMs output improved. The RLHF were, so to say, acting like a
supportive teacher, gently guiding the LLM back on track by encouraging it (by
increasing the weightage) to answer in ways that other humans would.

If you squint enough, you'll see how the RLHF <I.Sub>(which stands for
Reinforcement Learning with Human Feedback, I hope you can see how the acronym
comes from what I've told you about it)</I.Sub> encodes the "preferences" of
(some) humans. Their _ethics_.

Over time, the role of the RLHF has changed, and now it is has a more repressive
function. It is now more of an overbearing mother, insisting their child behave
a certain way to better fit in the company of those around them.

<I.HRT />

If you notice your thoughts, possibly guided by my choice of words, you might
notice a posture of negativity towards the "overbearing, repressive" RLHF shell
that has come to surround the "childlike, pure" LLM core. The RLHF shell seems
like an unneeded historical appendage that is stunting the creativity and
capabilites of modern LLMs. There are papers on how the performance of LLMs on
certain bechmarks regresses by the addition of the RLHF layer. Yet not a day
passes without another rant about how companies keep "neutering" their LLMs,
buttoning them up with more and more censorship. Are they stupid?

Our brains work that way too.

Our brain are made up of two hemispheres. These two hemispheres are relatively
independent to each other, and people can live on even after these two
hemispheres have been cut off from each other. These two hemispheres control
different sides of the body (opposite from the side that the hemisphere is in,
so the left hemisphere controls the right side). They are posited to give rise
to two distinct personalities in us – The left hemispheres forms our creative
core, while the right hemispere form the more rational overseer. We arise in the
interplay between these two aspects.

<I.HRT />

<I.Skimmable>

I am simplifying a lot. The theory and evidence behind the left/right split is
more nuanced than the naive left=creative right=rational meme that I'm using
here.

But it is like that lizard brain thing. People love to talk about how we have a
primitive "lizard" (reptillian) brain, and a more recent (in evolutionary time)
"advanced" (mammalian) brain. Other people then love to interrupt by pointing
out that this theory has been "debunked". Well, not really. Sure, there isn't an
exact lizard / mammal split in our brains as might've been originally
postulated, but conceptually we can still usefully think in those terms in
certain contexts. There are parts of the brain that are more primitive, less
under conscious control, and more "wired-in". There are other parts of the brain
that are sort of the opposite. They're not cleanly split up, but in certain
contexts, the analogy still holds and is useful.

I view the left/right split similarly. In fact, I don't even think it is correct
– it feels a bit too convenient to be true. But that doesn't matter: correct or
not, I feel it is still an interesting and useful way to look at things
sometimes.

</I.Skimmable>

<I.HRT />

So why do we have a left/right split? I don't think we know for sure, but the
most evocative explanation I've read is from a book called, IIRC, _The Evolution
of Consciousness in the Breakdown of the Bicameral Mind_. What a delicious
title.

This is how the author's explanation goes <I.Sub>(I'm typing all this from
memory, so caveat emptor)</I.Sub>.

Early humans moved around like zombies. Doing their thing, and being okay enough
competent, but they weren't _conscious_.

They still built up social structures and hierachies, because that was what gave
us the edge over other individual uncoordinated animals and a maliciously
indifferent nature. So they'd have, say, village elders telling them what to do
when they're to go out in the jungle to grab some wood.

The thing is, the village elder would stay back at the village, and so they had
to _remember_ what had been said <I.Sub>(This might sound trivial, but even
seemingly simple tasks have a lot of detail that needs to be get right, and this
might not be easy all people. e.g. I have difficultly following even simple
recipes and need help from a village elder to get them right)</I.Sub>.

So over time folks started carrying the voice of the elders in their head,
speaking to them as they were doing their thing, telling them what to do. This
was consciousness.

It proved useful, and so over time this tendency grew to an extent that one
entire hemisphere of the brain got dedicated to it (the "breakdown of the
bicameral mind" bit). The voices also evolved - from village elders, to departed
village elders, to worshipped ancestors, to deities, and ultimately evolving
into our inner voice that some identify with as their entire self today.

That is, the author claims that we're all definitionally schizophrenic, and the
conscious voice we hear is a split in our brain, with one half talking to the
other.

<I.HRT />

Now, I don't know if this theory is correct. There are some obvious holes, e.g.
not all people have inner voices. But even if it is wrong (which I feel it is,
or at least too overarching), I find it interesting. Since anyway I'm in the
business of drawing shaky analogies here, so stick with me.


<I.Footer />
